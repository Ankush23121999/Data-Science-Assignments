{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3303bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5e4d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine = pd.read_csv(\"gas_turbines.csv\")\n",
    "turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774d148f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0  6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1  6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2  6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3  7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4  7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "\n",
       "       CO     NOX  \n",
       "0  3.1547  82.722  \n",
       "1  3.2363  82.776  \n",
       "2  3.2012  82.468  \n",
       "3  3.1923  82.670  \n",
       "4  3.2484  82.311  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e6c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15039, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432d27a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      float64\n",
       "AP      float64\n",
       "AH      float64\n",
       "AFDP    float64\n",
       "GTEP    float64\n",
       "TIT     float64\n",
       "TAT     float64\n",
       "TEY     float64\n",
       "CDP     float64\n",
       "CO      float64\n",
       "NOX     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236f6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "turbine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a94a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      0\n",
       "AP      0\n",
       "AH      0\n",
       "AFDP    0\n",
       "GTEP    0\n",
       "TIT     0\n",
       "TAT     0\n",
       "TEY     0\n",
       "CDP     0\n",
       "CO      0\n",
       "NOX     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ddcdbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.18846399361655"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we have to convert the TEY column into categorical column so need to calculate the mean first and then using lambda function categorize that column into 0's and 1's according to the mean\n",
    "turbine_mean = turbine.TEY.mean()\n",
    "turbine_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c726deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine['performance'] = turbine.TEY.map(lambda x: 1 if x > 134 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8408f586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8484\n",
       "1    6555\n",
       "Name: performance, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.performance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a1a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  performance  \n",
       "0      3.1547  82.722            0  \n",
       "1      3.2363  82.776            0  \n",
       "2      3.2012  82.468            0  \n",
       "3      3.1923  82.670            0  \n",
       "4      3.2484  82.311            0  \n",
       "...       ...     ...          ...  \n",
       "15034  4.5186  79.559            0  \n",
       "15035  4.8470  79.917            0  \n",
       "15036  7.9632  90.912            0  \n",
       "15037  6.2494  93.227            0  \n",
       "15038  4.9816  92.498            0  \n",
       "\n",
       "[15039 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702015fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine.drop(['TEY'], inplace= True, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293d1a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX  performance  \n",
       "0      82.722            0  \n",
       "1      82.776            0  \n",
       "2      82.468            0  \n",
       "3      82.670            0  \n",
       "4      82.311            0  \n",
       "...       ...          ...  \n",
       "15034  79.559            0  \n",
       "15035  79.917            0  \n",
       "15036  90.912            0  \n",
       "15037  93.227            0  \n",
       "15038  92.498            0  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf0a07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15039, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9867f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into x and y as input and output\n",
    "\n",
    "X = turbine.iloc[:,0:10]\n",
    "Y = turbine.iloc[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca3e887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX  \n",
       "0      82.722  \n",
       "1      82.776  \n",
       "2      82.468  \n",
       "3      82.670  \n",
       "4      82.311  \n",
       "...       ...  \n",
       "15034  79.559  \n",
       "15035  79.917  \n",
       "15036  90.912  \n",
       "15037  93.227  \n",
       "15038  92.498  \n",
       "\n",
       "[15039 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d622cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "15034    0\n",
       "15035    0\n",
       "15036    0\n",
       "15037    0\n",
       "15038    0\n",
       "Name: performance, Length: 15039, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6e36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16ee4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training and test dataset\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size= 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1154bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b01ec883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now apply the transformations to the data:\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc34c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "885e43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ea80f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30, 30))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a068ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = mlp.predict(x_train)\n",
    "prediction_test = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccc906b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d915da5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeb7f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2446   94]\n",
      " [ 215 1757]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9498432601880877"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,prediction_test))\n",
    "np.mean(y_test==prediction_test)\n",
    "np.mean(y_train==prediction_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04a421",
   "metadata": {},
   "source": [
    "### Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5fa6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "985f79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10, activation= 'relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b0058a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5691aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1008/1008 [==============================] - 1s 897us/step - loss: 2.0065 - accuracy: 0.4222 - val_loss: 1.0481 - val_accuracy: 0.4390\n",
      "Epoch 2/150\n",
      "1008/1008 [==============================] - 1s 835us/step - loss: 0.7325 - accuracy: 0.6005 - val_loss: 0.6627 - val_accuracy: 0.5843\n",
      "Epoch 3/150\n",
      "1008/1008 [==============================] - 1s 811us/step - loss: 0.5373 - accuracy: 0.7242 - val_loss: 0.5144 - val_accuracy: 0.6943\n",
      "Epoch 4/150\n",
      "1008/1008 [==============================] - 1s 777us/step - loss: 0.4549 - accuracy: 0.7631 - val_loss: 0.3803 - val_accuracy: 0.8162\n",
      "Epoch 5/150\n",
      "1008/1008 [==============================] - 1s 778us/step - loss: 0.4196 - accuracy: 0.7767 - val_loss: 0.4126 - val_accuracy: 0.7411\n",
      "Epoch 6/150\n",
      "1008/1008 [==============================] - 1s 773us/step - loss: 0.3914 - accuracy: 0.7910 - val_loss: 0.3364 - val_accuracy: 0.8390\n",
      "Epoch 7/150\n",
      "1008/1008 [==============================] - 1s 840us/step - loss: 0.3864 - accuracy: 0.7896 - val_loss: 0.3173 - val_accuracy: 0.8507\n",
      "Epoch 8/150\n",
      "1008/1008 [==============================] - 1s 796us/step - loss: 0.3904 - accuracy: 0.7889 - val_loss: 0.3113 - val_accuracy: 0.8451\n",
      "Epoch 9/150\n",
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3790 - accuracy: 0.7969 - val_loss: 0.2994 - val_accuracy: 0.8727\n",
      "Epoch 10/150\n",
      "1008/1008 [==============================] - 1s 875us/step - loss: 0.3762 - accuracy: 0.8015 - val_loss: 0.3915 - val_accuracy: 0.7356\n",
      "Epoch 11/150\n",
      "1008/1008 [==============================] - 1s 810us/step - loss: 0.3725 - accuracy: 0.8025 - val_loss: 0.3453 - val_accuracy: 0.7487\n",
      "Epoch 12/150\n",
      "1008/1008 [==============================] - 1s 876us/step - loss: 0.3598 - accuracy: 0.8074 - val_loss: 0.3748 - val_accuracy: 0.8267\n",
      "Epoch 13/150\n",
      "1008/1008 [==============================] - 1s 809us/step - loss: 0.3757 - accuracy: 0.7953 - val_loss: 0.3868 - val_accuracy: 0.8237\n",
      "Epoch 14/150\n",
      "1008/1008 [==============================] - 1s 780us/step - loss: 0.3651 - accuracy: 0.8061 - val_loss: 0.3308 - val_accuracy: 0.7538\n",
      "Epoch 15/150\n",
      "1008/1008 [==============================] - 1s 803us/step - loss: 0.3627 - accuracy: 0.7999 - val_loss: 0.3108 - val_accuracy: 0.7886\n",
      "Epoch 16/150\n",
      "1008/1008 [==============================] - 1s 796us/step - loss: 0.3587 - accuracy: 0.8031 - val_loss: 0.2773 - val_accuracy: 0.8678\n",
      "Epoch 17/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3584 - accuracy: 0.8070 - val_loss: 0.2920 - val_accuracy: 0.8447\n",
      "Epoch 18/150\n",
      "1008/1008 [==============================] - 1s 797us/step - loss: 0.3506 - accuracy: 0.8133 - val_loss: 0.3014 - val_accuracy: 0.8412\n",
      "Epoch 19/150\n",
      "1008/1008 [==============================] - 1s 784us/step - loss: 0.3517 - accuracy: 0.8111 - val_loss: 0.2856 - val_accuracy: 0.8785\n",
      "Epoch 20/150\n",
      "1008/1008 [==============================] - 1s 862us/step - loss: 0.3513 - accuracy: 0.8076 - val_loss: 0.2802 - val_accuracy: 0.8479\n",
      "Epoch 21/150\n",
      "1008/1008 [==============================] - 1s 847us/step - loss: 0.3476 - accuracy: 0.8122 - val_loss: 0.2665 - val_accuracy: 0.8582\n",
      "Epoch 22/150\n",
      "1008/1008 [==============================] - 1s 863us/step - loss: 0.3516 - accuracy: 0.8091 - val_loss: 0.2951 - val_accuracy: 0.8424\n",
      "Epoch 23/150\n",
      "1008/1008 [==============================] - 1s 823us/step - loss: 0.3467 - accuracy: 0.8136 - val_loss: 0.2653 - val_accuracy: 0.8555\n",
      "Epoch 24/150\n",
      "1008/1008 [==============================] - 1s 839us/step - loss: 0.3423 - accuracy: 0.8148 - val_loss: 0.2641 - val_accuracy: 0.8535\n",
      "Epoch 25/150\n",
      "1008/1008 [==============================] - 1s 803us/step - loss: 0.3419 - accuracy: 0.8117 - val_loss: 0.4127 - val_accuracy: 0.7340\n",
      "Epoch 26/150\n",
      "1008/1008 [==============================] - 1s 785us/step - loss: 0.3451 - accuracy: 0.8124 - val_loss: 0.3508 - val_accuracy: 0.8356\n",
      "Epoch 27/150\n",
      "1008/1008 [==============================] - 1s 861us/step - loss: 0.3412 - accuracy: 0.8174 - val_loss: 0.2570 - val_accuracy: 0.8594\n",
      "Epoch 28/150\n",
      "1008/1008 [==============================] - 1s 813us/step - loss: 0.3400 - accuracy: 0.8147 - val_loss: 0.2812 - val_accuracy: 0.8461\n",
      "Epoch 29/150\n",
      "1008/1008 [==============================] - 1s 799us/step - loss: 0.3413 - accuracy: 0.8127 - val_loss: 0.2614 - val_accuracy: 0.8535\n",
      "Epoch 30/150\n",
      "1008/1008 [==============================] - 1s 815us/step - loss: 0.3438 - accuracy: 0.8137 - val_loss: 0.2630 - val_accuracy: 0.8535\n",
      "Epoch 31/150\n",
      "1008/1008 [==============================] - 1s 871us/step - loss: 0.3376 - accuracy: 0.8145 - val_loss: 0.2525 - val_accuracy: 0.8682\n",
      "Epoch 32/150\n",
      "1008/1008 [==============================] - 1s 842us/step - loss: 0.3446 - accuracy: 0.8122 - val_loss: 0.2612 - val_accuracy: 0.9146\n",
      "Epoch 33/150\n",
      "1008/1008 [==============================] - 1s 819us/step - loss: 0.3325 - accuracy: 0.8184 - val_loss: 0.2649 - val_accuracy: 0.8511\n",
      "Epoch 34/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3406 - accuracy: 0.8129 - val_loss: 0.2529 - val_accuracy: 0.8982\n",
      "Epoch 35/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3355 - accuracy: 0.8223 - val_loss: 0.2676 - val_accuracy: 0.8497\n",
      "Epoch 36/150\n",
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3335 - accuracy: 0.8221 - val_loss: 0.2603 - val_accuracy: 0.8527\n",
      "Epoch 37/150\n",
      "1008/1008 [==============================] - 1s 819us/step - loss: 0.3331 - accuracy: 0.8176 - val_loss: 0.2575 - val_accuracy: 0.9186\n",
      "Epoch 38/150\n",
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3411 - accuracy: 0.8171 - val_loss: 0.2492 - val_accuracy: 0.8712\n",
      "Epoch 39/150\n",
      "1008/1008 [==============================] - 1s 840us/step - loss: 0.3283 - accuracy: 0.8258 - val_loss: 0.2488 - val_accuracy: 0.8590\n",
      "Epoch 40/150\n",
      "1008/1008 [==============================] - 1s 842us/step - loss: 0.3283 - accuracy: 0.8224 - val_loss: 0.2452 - val_accuracy: 0.8638\n",
      "Epoch 41/150\n",
      "1008/1008 [==============================] - 1s 827us/step - loss: 0.3337 - accuracy: 0.8171 - val_loss: 0.2548 - val_accuracy: 0.8543\n",
      "Epoch 42/150\n",
      "1008/1008 [==============================] - 1s 836us/step - loss: 0.3352 - accuracy: 0.8217 - val_loss: 0.2503 - val_accuracy: 0.9178\n",
      "Epoch 43/150\n",
      "1008/1008 [==============================] - 1s 809us/step - loss: 0.3327 - accuracy: 0.8212 - val_loss: 0.2613 - val_accuracy: 0.9013\n",
      "Epoch 44/150\n",
      "1008/1008 [==============================] - 1s 843us/step - loss: 0.3364 - accuracy: 0.8220 - val_loss: 0.2657 - val_accuracy: 0.8896\n",
      "Epoch 45/150\n",
      "1008/1008 [==============================] - 1s 833us/step - loss: 0.3313 - accuracy: 0.8201 - val_loss: 0.2942 - val_accuracy: 0.7772\n",
      "Epoch 46/150\n",
      "1008/1008 [==============================] - 1s 796us/step - loss: 0.3287 - accuracy: 0.8251 - val_loss: 0.2579 - val_accuracy: 0.8537\n",
      "Epoch 47/150\n",
      "1008/1008 [==============================] - 1s 826us/step - loss: 0.3199 - accuracy: 0.8306 - val_loss: 0.2579 - val_accuracy: 0.8986\n",
      "Epoch 48/150\n",
      "1008/1008 [==============================] - 1s 820us/step - loss: 0.3355 - accuracy: 0.8239 - val_loss: 0.3327 - val_accuracy: 0.8396\n",
      "Epoch 49/150\n",
      "1008/1008 [==============================] - 1s 916us/step - loss: 0.3279 - accuracy: 0.8277 - val_loss: 0.3232 - val_accuracy: 0.8406\n",
      "Epoch 50/150\n",
      "1008/1008 [==============================] - 1s 817us/step - loss: 0.3223 - accuracy: 0.8294 - val_loss: 0.2391 - val_accuracy: 0.8811\n",
      "Epoch 51/150\n",
      "1008/1008 [==============================] - 1s 797us/step - loss: 0.3306 - accuracy: 0.8219 - val_loss: 0.2634 - val_accuracy: 0.8765\n",
      "Epoch 52/150\n",
      "1008/1008 [==============================] - 1s 800us/step - loss: 0.3215 - accuracy: 0.8239 - val_loss: 0.2640 - val_accuracy: 0.8515\n",
      "Epoch 53/150\n",
      "1008/1008 [==============================] - 1s 807us/step - loss: 0.3359 - accuracy: 0.8211 - val_loss: 0.2488 - val_accuracy: 0.8567\n",
      "Epoch 54/150\n",
      "1008/1008 [==============================] - 1s 826us/step - loss: 0.3204 - accuracy: 0.8301 - val_loss: 0.2563 - val_accuracy: 0.9015\n",
      "Epoch 55/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3258 - accuracy: 0.8238 - val_loss: 0.3866 - val_accuracy: 0.7413\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3221 - accuracy: 0.8282 - val_loss: 0.2545 - val_accuracy: 0.8537\n",
      "Epoch 57/150\n",
      "1008/1008 [==============================] - 1s 832us/step - loss: 0.3247 - accuracy: 0.8266 - val_loss: 0.2392 - val_accuracy: 0.8688\n",
      "Epoch 58/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3212 - accuracy: 0.8288 - val_loss: 0.2521 - val_accuracy: 0.8541\n",
      "Epoch 59/150\n",
      "1008/1008 [==============================] - 1s 834us/step - loss: 0.3292 - accuracy: 0.8242 - val_loss: 0.2572 - val_accuracy: 0.8527\n",
      "Epoch 60/150\n",
      "1008/1008 [==============================] - 1s 869us/step - loss: 0.3252 - accuracy: 0.8227 - val_loss: 0.2412 - val_accuracy: 0.9136\n",
      "Epoch 61/150\n",
      "1008/1008 [==============================] - 1s 933us/step - loss: 0.3175 - accuracy: 0.8332 - val_loss: 0.2664 - val_accuracy: 0.8501\n",
      "Epoch 62/150\n",
      "1008/1008 [==============================] - 1s 823us/step - loss: 0.3228 - accuracy: 0.8270 - val_loss: 0.2530 - val_accuracy: 0.8539\n",
      "Epoch 63/150\n",
      "1008/1008 [==============================] - 1s 855us/step - loss: 0.3283 - accuracy: 0.8219 - val_loss: 0.3130 - val_accuracy: 0.8436\n",
      "Epoch 64/150\n",
      "1008/1008 [==============================] - 1s 861us/step - loss: 0.3177 - accuracy: 0.8282 - val_loss: 0.2414 - val_accuracy: 0.8634\n",
      "Epoch 65/150\n",
      "1008/1008 [==============================] - 1s 842us/step - loss: 0.3147 - accuracy: 0.8341 - val_loss: 0.2664 - val_accuracy: 0.8503\n",
      "Epoch 66/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3245 - accuracy: 0.8288 - val_loss: 0.2447 - val_accuracy: 0.8565\n",
      "Epoch 67/150\n",
      "1008/1008 [==============================] - 1s 812us/step - loss: 0.3240 - accuracy: 0.8248 - val_loss: 0.2744 - val_accuracy: 0.8128\n",
      "Epoch 68/150\n",
      "1008/1008 [==============================] - 1s 791us/step - loss: 0.3224 - accuracy: 0.8297 - val_loss: 0.2439 - val_accuracy: 0.8573\n",
      "Epoch 69/150\n",
      "1008/1008 [==============================] - 1s 777us/step - loss: 0.3206 - accuracy: 0.8273 - val_loss: 0.3846 - val_accuracy: 0.8376\n",
      "Epoch 70/150\n",
      "1008/1008 [==============================] - 1s 824us/step - loss: 0.3204 - accuracy: 0.8281 - val_loss: 0.2441 - val_accuracy: 0.9279\n",
      "Epoch 71/150\n",
      "1008/1008 [==============================] - 1s 817us/step - loss: 0.3230 - accuracy: 0.8245 - val_loss: 0.2395 - val_accuracy: 0.8628\n",
      "Epoch 72/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3244 - accuracy: 0.8273 - val_loss: 0.2423 - val_accuracy: 0.8584\n",
      "Epoch 73/150\n",
      "1008/1008 [==============================] - 1s 823us/step - loss: 0.3189 - accuracy: 0.8276 - val_loss: 0.2475 - val_accuracy: 0.9204\n",
      "Epoch 74/150\n",
      "1008/1008 [==============================] - 1s 818us/step - loss: 0.3189 - accuracy: 0.8344 - val_loss: 0.2721 - val_accuracy: 0.8146\n",
      "Epoch 75/150\n",
      "1008/1008 [==============================] - 1s 839us/step - loss: 0.3128 - accuracy: 0.8346 - val_loss: 0.2332 - val_accuracy: 0.8787\n",
      "Epoch 76/150\n",
      "1008/1008 [==============================] - 1s 804us/step - loss: 0.3208 - accuracy: 0.8284 - val_loss: 0.2719 - val_accuracy: 0.8495\n",
      "Epoch 77/150\n",
      "1008/1008 [==============================] - 1s 820us/step - loss: 0.3226 - accuracy: 0.8256 - val_loss: 0.2341 - val_accuracy: 0.8954\n",
      "Epoch 78/150\n",
      "1008/1008 [==============================] - 1s 819us/step - loss: 0.3177 - accuracy: 0.8265 - val_loss: 0.2566 - val_accuracy: 0.8775\n",
      "Epoch 79/150\n",
      "1008/1008 [==============================] - 1s 856us/step - loss: 0.3199 - accuracy: 0.8281 - val_loss: 0.2595 - val_accuracy: 0.8626\n",
      "Epoch 80/150\n",
      "1008/1008 [==============================] - 1s 779us/step - loss: 0.3203 - accuracy: 0.8305 - val_loss: 0.2404 - val_accuracy: 0.9263\n",
      "Epoch 81/150\n",
      "1008/1008 [==============================] - 1s 779us/step - loss: 0.3193 - accuracy: 0.8311 - val_loss: 0.2342 - val_accuracy: 0.8900\n",
      "Epoch 82/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3179 - accuracy: 0.8289 - val_loss: 0.2350 - val_accuracy: 0.8664\n",
      "Epoch 83/150\n",
      "1008/1008 [==============================] - 1s 817us/step - loss: 0.3214 - accuracy: 0.8302 - val_loss: 0.2556 - val_accuracy: 0.8535\n",
      "Epoch 84/150\n",
      "1008/1008 [==============================] - 1s 828us/step - loss: 0.3118 - accuracy: 0.8353 - val_loss: 0.3148 - val_accuracy: 0.8451\n",
      "Epoch 85/150\n",
      "1008/1008 [==============================] - 1s 786us/step - loss: 0.3116 - accuracy: 0.8364 - val_loss: 0.2393 - val_accuracy: 0.8590\n",
      "Epoch 86/150\n",
      "1008/1008 [==============================] - 1s 836us/step - loss: 0.3128 - accuracy: 0.8334 - val_loss: 0.2371 - val_accuracy: 0.9267\n",
      "Epoch 87/150\n",
      "1008/1008 [==============================] - 1s 840us/step - loss: 0.3109 - accuracy: 0.8326 - val_loss: 0.2376 - val_accuracy: 0.8604\n",
      "Epoch 88/150\n",
      "1008/1008 [==============================] - 1s 781us/step - loss: 0.3101 - accuracy: 0.8378 - val_loss: 0.2290 - val_accuracy: 0.8926\n",
      "Epoch 89/150\n",
      "1008/1008 [==============================] - 1s 809us/step - loss: 0.3216 - accuracy: 0.8273 - val_loss: 0.2521 - val_accuracy: 0.8543\n",
      "Epoch 90/150\n",
      "1008/1008 [==============================] - 1s 785us/step - loss: 0.3115 - accuracy: 0.8356 - val_loss: 0.2905 - val_accuracy: 0.8475\n",
      "Epoch 91/150\n",
      "1008/1008 [==============================] - 1s 855us/step - loss: 0.3203 - accuracy: 0.8286 - val_loss: 0.2347 - val_accuracy: 0.8664\n",
      "Epoch 92/150\n",
      "1008/1008 [==============================] - 1s 817us/step - loss: 0.3114 - accuracy: 0.8339 - val_loss: 0.3060 - val_accuracy: 0.8459\n",
      "Epoch 93/150\n",
      "1008/1008 [==============================] - 1s 807us/step - loss: 0.3195 - accuracy: 0.8335 - val_loss: 0.3154 - val_accuracy: 0.8451\n",
      "Epoch 94/150\n",
      "1008/1008 [==============================] - 1s 803us/step - loss: 0.3143 - accuracy: 0.8322 - val_loss: 0.3676 - val_accuracy: 0.8406\n",
      "Epoch 95/150\n",
      "1008/1008 [==============================] - 1s 782us/step - loss: 0.3181 - accuracy: 0.8293 - val_loss: 0.2383 - val_accuracy: 0.8612\n",
      "Epoch 96/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3212 - accuracy: 0.8262 - val_loss: 0.2657 - val_accuracy: 0.8511\n",
      "Epoch 97/150\n",
      "1008/1008 [==============================] - 1s 785us/step - loss: 0.3088 - accuracy: 0.8389 - val_loss: 0.2323 - val_accuracy: 0.8656\n",
      "Epoch 98/150\n",
      "1008/1008 [==============================] - 1s 813us/step - loss: 0.3153 - accuracy: 0.8319 - val_loss: 0.2488 - val_accuracy: 0.9031\n",
      "Epoch 99/150\n",
      "1008/1008 [==============================] - 1s 850us/step - loss: 0.3191 - accuracy: 0.8298 - val_loss: 0.2489 - val_accuracy: 0.8549\n",
      "Epoch 100/150\n",
      "1008/1008 [==============================] - 1s 885us/step - loss: 0.3109 - accuracy: 0.8369 - val_loss: 0.2295 - val_accuracy: 0.8787\n",
      "Epoch 101/150\n",
      "1008/1008 [==============================] - 1s 859us/step - loss: 0.3116 - accuracy: 0.8368 - val_loss: 0.3033 - val_accuracy: 0.8461\n",
      "Epoch 102/150\n",
      "1008/1008 [==============================] - 1s 792us/step - loss: 0.3108 - accuracy: 0.8338 - val_loss: 0.2338 - val_accuracy: 0.9250\n",
      "Epoch 103/150\n",
      "1008/1008 [==============================] - 1s 785us/step - loss: 0.3065 - accuracy: 0.8378 - val_loss: 0.2277 - val_accuracy: 0.8700\n",
      "Epoch 104/150\n",
      "1008/1008 [==============================] - 1s 793us/step - loss: 0.3196 - accuracy: 0.8290 - val_loss: 0.2290 - val_accuracy: 0.8827\n",
      "Epoch 105/150\n",
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3096 - accuracy: 0.8301 - val_loss: 0.2449 - val_accuracy: 0.8569\n",
      "Epoch 106/150\n",
      "1008/1008 [==============================] - 1s 824us/step - loss: 0.3138 - accuracy: 0.8306 - val_loss: 0.2381 - val_accuracy: 0.8598\n",
      "Epoch 107/150\n",
      "1008/1008 [==============================] - 1s 800us/step - loss: 0.3121 - accuracy: 0.8336 - val_loss: 0.2285 - val_accuracy: 0.8896\n",
      "Epoch 108/150\n",
      "1008/1008 [==============================] - 1s 803us/step - loss: 0.3129 - accuracy: 0.8360 - val_loss: 0.2484 - val_accuracy: 0.8553\n",
      "Epoch 109/150\n",
      "1008/1008 [==============================] - 1s 857us/step - loss: 0.3129 - accuracy: 0.8337 - val_loss: 0.2470 - val_accuracy: 0.8563\n",
      "Epoch 110/150\n",
      "1008/1008 [==============================] - 1s 870us/step - loss: 0.3206 - accuracy: 0.8282 - val_loss: 0.2352 - val_accuracy: 0.8648\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008/1008 [==============================] - 1s 802us/step - loss: 0.3086 - accuracy: 0.8350 - val_loss: 0.2318 - val_accuracy: 0.9142\n",
      "Epoch 112/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3130 - accuracy: 0.8356 - val_loss: 0.2417 - val_accuracy: 0.9220\n",
      "Epoch 113/150\n",
      "1008/1008 [==============================] - 1s 827us/step - loss: 0.3067 - accuracy: 0.8329 - val_loss: 0.3223 - val_accuracy: 0.8453\n",
      "Epoch 114/150\n",
      "1008/1008 [==============================] - 1s 799us/step - loss: 0.3131 - accuracy: 0.8331 - val_loss: 0.3036 - val_accuracy: 0.8465\n",
      "Epoch 115/150\n",
      "1008/1008 [==============================] - 1s 827us/step - loss: 0.3092 - accuracy: 0.8395 - val_loss: 0.2320 - val_accuracy: 0.9228\n",
      "Epoch 116/150\n",
      "1008/1008 [==============================] - 1s 833us/step - loss: 0.3143 - accuracy: 0.8348 - val_loss: 0.2794 - val_accuracy: 0.7925\n",
      "Epoch 117/150\n",
      "1008/1008 [==============================] - 1s 811us/step - loss: 0.3114 - accuracy: 0.8394 - val_loss: 0.2680 - val_accuracy: 0.8189\n",
      "Epoch 118/150\n",
      "1008/1008 [==============================] - 1s 830us/step - loss: 0.3071 - accuracy: 0.8370 - val_loss: 0.2712 - val_accuracy: 0.8092\n",
      "Epoch 119/150\n",
      "1008/1008 [==============================] - 1s 820us/step - loss: 0.3075 - accuracy: 0.8329 - val_loss: 0.2294 - val_accuracy: 0.8717\n",
      "Epoch 120/150\n",
      "1008/1008 [==============================] - 1s 791us/step - loss: 0.3110 - accuracy: 0.8333 - val_loss: 0.2718 - val_accuracy: 0.8511\n",
      "Epoch 121/150\n",
      "1008/1008 [==============================] - 1s 827us/step - loss: 0.3055 - accuracy: 0.8386 - val_loss: 0.2232 - val_accuracy: 0.8960\n",
      "Epoch 122/150\n",
      "1008/1008 [==============================] - 1s 839us/step - loss: 0.3136 - accuracy: 0.8345 - val_loss: 0.2640 - val_accuracy: 0.8275\n",
      "Epoch 123/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3129 - accuracy: 0.8338 - val_loss: 0.2584 - val_accuracy: 0.8541\n",
      "Epoch 124/150\n",
      "1008/1008 [==============================] - 1s 814us/step - loss: 0.3115 - accuracy: 0.8332 - val_loss: 0.2459 - val_accuracy: 0.9067\n",
      "Epoch 125/150\n",
      "1008/1008 [==============================] - 1s 877us/step - loss: 0.3071 - accuracy: 0.8355 - val_loss: 0.2540 - val_accuracy: 0.8547\n",
      "Epoch 126/150\n",
      "1008/1008 [==============================] - 1s 822us/step - loss: 0.3100 - accuracy: 0.8317 - val_loss: 0.2557 - val_accuracy: 0.8598\n",
      "Epoch 127/150\n",
      "1008/1008 [==============================] - 1s 845us/step - loss: 0.3012 - accuracy: 0.8395 - val_loss: 0.2511 - val_accuracy: 0.8557\n",
      "Epoch 128/150\n",
      "1008/1008 [==============================] - 1s 804us/step - loss: 0.3057 - accuracy: 0.8399 - val_loss: 0.2233 - val_accuracy: 0.8954\n",
      "Epoch 129/150\n",
      "1008/1008 [==============================] - 1s 804us/step - loss: 0.2994 - accuracy: 0.8400 - val_loss: 0.2351 - val_accuracy: 0.8622\n",
      "Epoch 130/150\n",
      "1008/1008 [==============================] - 1s 811us/step - loss: 0.3070 - accuracy: 0.8409 - val_loss: 0.2289 - val_accuracy: 0.9303\n",
      "Epoch 131/150\n",
      "1008/1008 [==============================] - 1s 829us/step - loss: 0.3008 - accuracy: 0.8421 - val_loss: 0.2303 - val_accuracy: 0.8644\n",
      "Epoch 132/150\n",
      "1008/1008 [==============================] - 1s 808us/step - loss: 0.3149 - accuracy: 0.8300 - val_loss: 0.2286 - val_accuracy: 0.9216\n",
      "Epoch 133/150\n",
      "1008/1008 [==============================] - 1s 818us/step - loss: 0.2994 - accuracy: 0.8458 - val_loss: 0.2255 - val_accuracy: 0.9222\n",
      "Epoch 134/150\n",
      "1008/1008 [==============================] - 1s 825us/step - loss: 0.3061 - accuracy: 0.8381 - val_loss: 0.2257 - val_accuracy: 0.8797\n",
      "Epoch 135/150\n",
      "1008/1008 [==============================] - 1s 809us/step - loss: 0.3096 - accuracy: 0.8395 - val_loss: 0.2244 - val_accuracy: 0.9101\n",
      "Epoch 136/150\n",
      "1008/1008 [==============================] - 1s 798us/step - loss: 0.3099 - accuracy: 0.8379 - val_loss: 0.2539 - val_accuracy: 0.8561\n",
      "Epoch 137/150\n",
      "1008/1008 [==============================] - 1s 827us/step - loss: 0.3094 - accuracy: 0.8369 - val_loss: 0.2305 - val_accuracy: 0.8884\n",
      "Epoch 138/150\n",
      "1008/1008 [==============================] - 1s 819us/step - loss: 0.3096 - accuracy: 0.8328 - val_loss: 0.2634 - val_accuracy: 0.8414\n",
      "Epoch 139/150\n",
      "1008/1008 [==============================] - 1s 779us/step - loss: 0.3077 - accuracy: 0.8366 - val_loss: 0.2536 - val_accuracy: 0.8809\n",
      "Epoch 140/150\n",
      "1008/1008 [==============================] - 1s 833us/step - loss: 0.3083 - accuracy: 0.8376 - val_loss: 0.2388 - val_accuracy: 0.8579\n",
      "Epoch 141/150\n",
      "1008/1008 [==============================] - 1s 835us/step - loss: 0.3110 - accuracy: 0.8385 - val_loss: 0.2860 - val_accuracy: 0.8465\n",
      "Epoch 142/150\n",
      "1008/1008 [==============================] - 1s 813us/step - loss: 0.3087 - accuracy: 0.8345 - val_loss: 0.2317 - val_accuracy: 0.8735\n",
      "Epoch 143/150\n",
      "1008/1008 [==============================] - 1s 794us/step - loss: 0.3138 - accuracy: 0.8298 - val_loss: 0.2706 - val_accuracy: 0.8491\n",
      "Epoch 144/150\n",
      "1008/1008 [==============================] - 1s 812us/step - loss: 0.3161 - accuracy: 0.8306 - val_loss: 0.2314 - val_accuracy: 0.8864\n",
      "Epoch 145/150\n",
      "1008/1008 [==============================] - 1s 806us/step - loss: 0.3067 - accuracy: 0.8389 - val_loss: 0.2313 - val_accuracy: 0.8731\n",
      "Epoch 146/150\n",
      "1008/1008 [==============================] - 1s 796us/step - loss: 0.3108 - accuracy: 0.8314 - val_loss: 0.2387 - val_accuracy: 0.8598\n",
      "Epoch 147/150\n",
      "1008/1008 [==============================] - 1s 800us/step - loss: 0.3070 - accuracy: 0.8330 - val_loss: 0.2325 - val_accuracy: 0.8910\n",
      "Epoch 148/150\n",
      "1008/1008 [==============================] - 1s 792us/step - loss: 0.3064 - accuracy: 0.8363 - val_loss: 0.2327 - val_accuracy: 0.9041\n",
      "Epoch 149/150\n",
      "1008/1008 [==============================] - 1s 904us/step - loss: 0.3007 - accuracy: 0.8404 - val_loss: 0.2468 - val_accuracy: 0.8565\n",
      "Epoch 150/150\n",
      "1008/1008 [==============================] - 1s 862us/step - loss: 0.3066 - accuracy: 0.8303 - val_loss: 0.2428 - val_accuracy: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbc5377e50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(X,Y , validation_split= 0.33, epochs=150, batch_size =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3c20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 509us/step - loss: 0.2889 - accuracy: 0.8734\n",
      "accuracy: 87.34%\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "scores = model.evaluate(X,Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688c25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
